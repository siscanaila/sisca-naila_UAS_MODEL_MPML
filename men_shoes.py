# -*- coding: utf-8 -*-
"""Customer online foods.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R5Itv4i7vZrKUM3QMlZmiuqiesSovlrF
"""

import pandas as pd

# Memuat dataset
df = pd.read_csv('/content/MEN_SHOES.csv')

# Menampilkan 5 baris pertama dari dataset
df.head()

df.describe()

df.info()

from google.colab import drive
drive.mount('/content/drive')

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Visualisasi distribusi kolom numerik (RATING)
df['RATING'].hist(bins=20, figsize=(8, 6))
plt.title('Distribution of RATING')
plt.xlabel('RATING')
plt.ylabel('Frequency')
plt.show()

# Clean and convert 'How_Many_Sold' to numeric
# Remove commas and convert to numeric, coercing errors to NaN
df['How_Many_Sold_Numeric'] = pd.to_numeric(df['How_Many_Sold'].str.replace(',', ''), errors='coerce')

# Clean and convert 'Current_Price' to numeric
# Remove currency symbols and commas, then convert to numeric, coercing errors to NaN
df['Current_Price_Numeric'] = pd.to_numeric(df['Current_Price'].astype(str).str.replace('â‚¹', '').str.replace(',', ''), errors='coerce')

# Select only the numeric columns for correlation
numeric_cols_for_corr = df[['RATING', 'How_Many_Sold_Numeric', 'Current_Price_Numeric']]

# Drop rows with NaN values that resulted from coercion
numeric_cols_for_corr = numeric_cols_for_corr.dropna()

# Cek korelasi antar fitur numerik
plt.figure(figsize=(8, 6))
sns.heatmap(numeric_cols_for_corr.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix of Numeric Features')
plt.show()

missing_values = df.isnull().sum()
missing_values

# 1. Bersihkan dan konversi ke numerik
def clean_price(price):
    if pd.isna(price):
        return np.nan
    # Hapus karakter non-digit (termasuk simbol mata uang, koma, dll)
    cleaned = ''.join(filter(str.isdigit, str(price)))
    return float(cleaned) if cleaned else np.nan

df['Current_Price_Numeric'] = df['Current_Price'].apply(clean_price)

# 2. Hitung mean
mean_price = df['Current_Price_Numeric'].mean()

# 3. Isi missing values
df['Current_Price_Numeric'].fillna(mean_price, inplace=True)

# 4. Verifikasi
print(f"Mean Price: {mean_price}")
print(f"Missing values setelah pengisian: {df['Current_Price_Numeric'].isnull().sum()}")
print("\nData setelah cleaning:")
print(df[['Current_Price', 'Current_Price_Numeric']])

# Tambahkan kolom baru yang sudah dibersihkan
df['Current_Price_Cleaned'] = df['Current_Price_Numeric']

import pandas as pd

# Clean and convert 'How_Many_Sold' to numeric
# Remove commas and convert to numeric, coercing errors to NaN
df['How_Many_Sold_Numeric'] = pd.to_numeric(df['How_Many_Sold'].str.replace(',', ''), errors='coerce')

# Display the first few rows to verify the new column
print("DataFrame with 'How_Many_Sold_Numeric' column:")
display(df[['How_Many_Sold', 'How_Many_Sold_Numeric']].head())

# Check for any NaNs introduced by coercion
nan_count = df['How_Many_Sold_Numeric'].isnull().sum()
print(f"\nNumber of NaN values in 'How_Many_Sold_Numeric' after coercion: {nan_count}")

df.drop('Current_Price', axis=1, inplace=True)

missing_values = df.isnull().sum()
missing_values

from sklearn.model_selection import train_test_split
import pandas as pd

# Asumsi Anda sudah memiliki DataFrame 'df' dengan kolom 'Current_Price_Numeric' dan 'How_Many_Sold_Numeric'
# yang sudah dibersihkan dan diubah ke numerik.

# Menggunakan 'Current_Price_Numeric' sebagai fitur (X)
X = df[['Current_Price_Numeric']] # Gunakan double brackets untuk memastikan X adalah DataFrame

# Menggunakan 'How_Many_Sold_Numeric' sebagai target (y)
y = df['How_Many_Sold_Numeric']

# Drop rows where either X or y is NaN to ensure consistency after previous cleaning steps
# Although fillna was used for Current_Price_Numeric, How_Many_Sold_Numeric might still have NaNs from coercion
# Check for NaNs in the selected X and y before splitting
combined_data = pd.concat([X, y], axis=1).dropna()
X = combined_data[['Current_Price_Numeric']]
y = combined_data['How_Many_Sold_Numeric']


# Membagi dataset menjadi training dan testing set (misalnya 80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Verifikasi shape dari fitur dan target setelah splitting
print(f"Shape of training features (X_train): {X_train.shape}")
print(f"Shape of testing features (X_test): {X_test.shape}")
print(f"Shape of training target (y_train): {y_train.shape}")
print(f"Shape of testing target (y_test): {y_test.shape}")

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Inisialisasi model-model regresi
linear_reg_model = LinearRegression()
dt_reg_model = DecisionTreeRegressor(random_state=42)
rf_reg_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Melatih model
print("Training Linear Regression Model...")
linear_reg_model.fit(X_train, y_train)
print("Training Decision Tree Regressor Model...")
dt_reg_model.fit(X_train, y_train)
print("Training Random Forest Regressor Model...")
rf_reg_model.fit(X_train, y_train)

# Prediksi pada data test
linear_reg_pred = linear_reg_model.predict(X_test)
dt_reg_pred = dt_reg_model.predict(X_test)
rf_reg_pred = rf_reg_model.predict(X_test)

# Evaluasi model menggunakan metrik regresi
print("\n--- Model Evaluation ---")

# Linear Regression
print("Linear Regression Model:")
print(f"  MAE: {mean_absolute_error(y_test, linear_reg_pred):.4f}")
print(f"  MSE: {mean_squared_error(y_test, linear_reg_pred):.4f}")
print(f"  RMSE: {np.sqrt(mean_squared_error(y_test, linear_reg_pred)):.4f}")
print(f"  R-squared: {r2_score(y_test, linear_reg_pred):.4f}")
print("-" * 30)

# Decision Tree Regressor
print("Decision Tree Regressor Model:")
print(f"  MAE: {mean_absolute_error(y_test, dt_reg_pred):.4f}")
print(f"  MSE: {mean_squared_error(y_test, dt_reg_pred):.4f}")
print(f"  RMSE: {np.sqrt(mean_squared_error(y_test, dt_reg_pred)):.4f}")
print(f"  R-squared: {r2_score(y_test, dt_reg_pred):.4f}")
print("-" * 30)

# Random Forest Regressor
print("Random Forest Regressor Model:")
print(f"  MAE: {mean_absolute_error(y_test, rf_reg_pred):.4f}")
print(f"  MSE: {mean_squared_error(y_test, rf_reg_pred):.4f}")
print(f"  RMSE: {np.sqrt(mean_squared_error(y_test, rf_reg_pred)):.4f}")
print(f"  R-squared: {r2_score(y_test, rf_reg_pred):.4f}")
print("-" * 30)

import matplotlib.pyplot as plt
import numpy as np

# Extract evaluation metrics from the output of cell cNiIscZS_nau
# Assuming cell cNiIscZS_nau has been executed and its output is available
models = ['Linear Regression', 'Decision Tree Regressor', 'Random Forest Regressor']

# Manually extract MAE and R-squared values from the output of cell cNiIscZS_nau
# Example values based on the provided output in cell cNiIscZS_nau:
mae_scores = [4950.6358, 257.3316, 257.2951] # Replace with actual values from your output if different
r2_scores = [0.0225, 0.9953, 0.9953]     # Replace with actual values from your output if different


# Membuat bar plot untuk MAE
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.bar(models, mae_scores, color=['blue', 'green', 'red'])
plt.title('Comparison of Model MAE')
plt.xlabel('Model')
plt.ylabel('Mean Absolute Error (MAE)')
# MAE: lower is better

# Membuat bar plot untuk R-squared
plt.subplot(1, 2, 2)
plt.bar(models, r2_scores, color=['blue', 'green', 'red'])
plt.title('Comparison of Model R-squared')
plt.xlabel('Model')
plt.ylabel('R-squared')
plt.ylim(0, 1) # R-squared is typically between 0 and 1
# R-squared: higher is better


plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Plot Actual vs. Predicted values for each regression model

plt.figure(figsize=(18, 5))

# Linear Regression
plt.subplot(1, 3, 1)
plt.scatter(y_test, linear_reg_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2) # Diagonal line
plt.xlabel('Actual How_Many_Sold_Numeric')
plt.ylabel('Predicted How_Many_Sold_Numeric')
plt.title('Linear Regression: Actual vs. Predicted')
plt.grid(True)


# Decision Tree Regressor
plt.subplot(1, 3, 2)
plt.scatter(y_test, dt_reg_pred, alpha=0.5, color='green')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2) # Diagonal line
plt.xlabel('Actual How_Many_Sold_Numeric')
plt.ylabel('Predicted How_Many_Sold_Numeric')
plt.title('Decision Tree Regressor: Actual vs. Predicted')
plt.grid(True)


# Random Forest Regressor
plt.subplot(1, 3, 3)
plt.scatter(y_test, rf_reg_pred, alpha=0.5, color='red')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2) # Diagonal line
plt.xlabel('Actual How_Many_Sold_Numeric')
plt.ylabel('Predicted How_Many_Sold_Numeric')
plt.title('Random Forest Regressor: Actual vs. Predicted')
plt.grid(True)

plt.tight_layout()
plt.show()

from sklearn.model_selection import KFold, cross_val_score
from sklearn.ensemble import RandomForestRegressor
import numpy as np

# Inisialisasi model Random Forest Regressor (gunakan parameter yang sama dengan model terbaik jika sudah ditentukan)
# Jika belum, gunakan parameter default atau parameter yang sudah Anda coba
rf_reg_model_cv = RandomForestRegressor(n_estimators=100, random_state=42)

# Inisialisasi K-Fold Cross-Validation
# KFold cocok untuk target regresi (numerik)
n_splits = 5 # Jumlah fold
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

# Lakukan cross-validation
# Menggunakan X_train dan y_train dari sel _vGkxSRdF1VL
cv_scores_rf = cross_val_score(rf_reg_model_cv, X_train, y_train, cv=kf, scoring='r2') # Menggunakan R-squared sebagai metrik

# Tampilkan hasil cross-validation
print("Random Forest Regressor Model Cross-validation R-squared Scores:")
print(cv_scores_rf)
print(f"Mean Random Forest Regressor Cross-validation R-squared: {cv_scores_rf.mean():.4f}")
print(f"Standard Deviation of Random Forest Regressor Cross-validation R-squared: {cv_scores_rf.std():.4f}")

from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, make_scorer
import numpy as np
from scipy.stats import randint # Untuk distribusi integer acak

# Inisialisasi model Random Forest Regressor
rf_reg_model_tune = RandomForestRegressor(random_state=42)

# Definisikan distribusi hyperparameter yang akan diambil sampelnya secara acak
param_dist_rf_reg = {
    'n_estimators': randint(100, 301),  # Jumlah pohon antara 100 dan 300
    'max_depth': [None, 10, 20, 30],  # Kedalaman maksimum pohon
    'min_samples_split': randint(2, 11),  # Jumlah minimal sampel untuk membagi antara 2 dan 10
    'min_samples_leaf': randint(1, 5)  # Jumlah minimal sampel di setiap daun antara 1 dan 4
}

# Membuat scorer dari metrik regresi (Negative Mean Squared Error)
neg_mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)

# Inisialisasi Stratified K-Fold Cross-Validation (digunakan oleh RandomizedSearchCV)
# Meskipun targetnya regresi, RandomizedSearchCV bisa menggunakan cv splitter apapun
# Kita bisa tetap menggunakan KFold atau KFold biasa jika tidak ada isu imbalance kelas
# Untuk konsistensi dengan sebelumnya, mari kita gunakan KFold saja karena targetnya numerik
from sklearn.model_selection import KFold
kf = KFold(n_splits=5, shuffle=True, random_state=42)


# Membuat objek RandomizedSearchCV
# n_iter menentukan berapa banyak kombinasi hyperparameter yang akan dicoba
# Menggunakan X_train dan y_train dari sel _vGkxSRdF1VL
random_search_rf_reg = RandomizedSearchCV(estimator=rf_reg_model_tune, param_distributions=param_dist_rf_reg,
                                         n_iter=10, cv=kf, scoring=neg_mse_scorer, n_jobs=-1, random_state=42) # n_iter=100 adalah jumlah kombinasi yang dicoba


# Melatih model dengan random search
print("Performing Random Search for Random Forest Regressor...")
random_search_rf_reg.fit(X_train, y_train)

# Menampilkan parameter terbaik dan skor terbaik
print("\nBest parameters for Random Forest Regressor:", random_search_rf_reg.best_params_)
# Skor terbaik adalah Negative MSE, jadi kita ambil nilai absolut untuk mendapatkan MSE
print(f"Best cross-validation MSE: {-random_search_rf_reg.best_score_:.4f}")

# Menerapkan model dengan parameter terbaik
best_rf_reg_model = random_search_rf_reg.best_estimator_

# Evaluasi dengan data uji
rf_reg_pred_tuned = best_rf_reg_model.predict(X_test)
print("\nRandom Forest Regressor Model Evaluation after tuning:")
print(f"  MAE: {mean_absolute_error(y_test, rf_reg_pred_tuned):.4f}")
print(f"  MSE: {mean_squared_error(y_test, rf_reg_pred_tuned):.4f}")
print(f"  RMSE: {np.sqrt(mean_squared_error(y_test, rf_reg_pred_tuned)):.4f}")
print(f"  R-squared: {r2_score(y_test, rf_reg_pred_tuned):.4f}")

from joblib import dump

# Simpan model ke file .joblib
dump(rf_reg_model, 'random_forest_regressor.joblib')

print("Model Random Forest Regressor telah disimpan sebagai 'random_forest_regressor.joblib'")

